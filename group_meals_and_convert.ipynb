{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to user the combined consumption user data and group it into meals, while using the portion weight conversion to use natural units in our output table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\qianc\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\qianc\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\qianc\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\qianc\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\qianc\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk-CPhZAdAE6U8i_gAmTQTgCEXiTNo76kHjjUcJGHrBW5T3BlbkFJcP17XqximvPPzxON2qWXlZB11ON22Ng316YOJmLwoA\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(verbose=True)\n",
        "\n",
        "# Set up OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "print(os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df = pd.read_csv('combined_consumption_user_with_days_and_food_name.csv')\n",
        "with open('Portion Weight Conversion/food_unit_conversion_with_embeddings.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "conversion_df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_grouped_meals_csv():\n",
        "    # Define the column names\n",
        "    columns = ['Meal Description', 'Meal Name', 'Serving Sizes', 'Meal Type', 'Country', 'Source', 'Weight', 'Carb', 'Protein', 'Fat', 'Fiber', 'Calories']\n",
        "\n",
        "    # Create the CSV file and write the header\n",
        "    with open('grouped_meals_final.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(columns)\n",
        "\n",
        "        # You can add some sample data here if you want\n",
        "        # For example:\n",
        "        # writer.writerow(['Delicious pasta dish', 'Spaghetti Bolognese', '1 plate', 'Dinner', 'Italy', 'Traditional', '65g'])\n",
        "\n",
        "    print(\"CSV file 'grouped_meals.csv' has been created with the specified columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('grouped_meals_final.csv'):\n",
        "    create_grouped_meals_csv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "meal_type_dict = {\n",
        "    1: \"Before breakfast\",\n",
        "    2: \"Breakfast\",\n",
        "    3: \"Snack or drink between breakfast and lunch\",\n",
        "    4: \"Lunch\",\n",
        "    5: \"Snack or drink between lunch and dinner\",\n",
        "    6: \"Dinner\",\n",
        "    7: \"Snack or drink after dinner\",\n",
        "    8: \"Snack or drink (unspecified when)\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_table_to_dict(table_string):\n",
        "    # Parse the table using pandas\n",
        "    df = pd.read_csv(StringIO(table_string), sep='|', skipinitialspace=True).iloc[1:]\n",
        "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "\n",
        "    # Convert to a dictionary\n",
        "    dish_dict = dict(zip(df['Dish'], df['Ingredients']))\n",
        "\n",
        "    # If you want to split the ingredients into a list\n",
        "    dish_dict_list = {dish: ingredients.split(', ') if isinstance(ingredients, str) else [] for dish, ingredients in dish_dict.items()}\n",
        "\n",
        "    return dish_dict_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def round_to_fraction(value):\n",
        "    quarters = round(value * 4) / 4\n",
        "    if quarters == 0:\n",
        "        return \"0\"  # or any other appropriate term for very small amounts\n",
        "    elif quarters < 1:\n",
        "        if quarters == 0.25:\n",
        "            return \"one fourth\"\n",
        "        elif quarters == 0.5:\n",
        "            return \"one half\"\n",
        "        elif quarters == 0.75:\n",
        "            return \"three fourths\"\n",
        "        else:   \n",
        "            return f\"{quarters:.2f}\".rstrip('0').rstrip('.')\n",
        "    else:\n",
        "        whole = int(quarters)\n",
        "        fraction = quarters - whole\n",
        "        if fraction == 0:\n",
        "            return f\"{whole}\"\n",
        "        elif fraction == 0.25:\n",
        "            return f\"{whole} 1/4\"\n",
        "        elif fraction == 0.5:\n",
        "            return f\"{whole} 1/2\"\n",
        "        elif fraction == 0.75:\n",
        "            return f\"{whole} 3/4\"\n",
        "        else:\n",
        "            return f\"{quarters:.2f}\"\n",
        "        \n",
        "def calculate_nutrition_similarity(food1, food2):\n",
        "    nutrients = ['ENERGY_kcal', 'PROTEIN_g', 'FAT_g', 'CARBOH_g']\n",
        "    similarity = 0\n",
        "    for nutrient in nutrients:\n",
        "        if nutrient in food1 and nutrient in food2:\n",
        "            similarity += 1 - abs(food1[nutrient] - food2[nutrient]) / max(food1[nutrient], food2[nutrient])\n",
        "    return similarity / len(nutrients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def portion_weight_conversion(meal_descriptions, meal_weights):\n",
        "    # Assuming conversion_df is already defined and has columns: 'grams', 'natural_unit', 'quantity', and 'embedding'\n",
        "    \n",
        "    def get_meal_embedding(meal):\n",
        "        try:\n",
        "            response = client.embeddings.create(\n",
        "                input=json.dumps(meal),  # Convert to JSON string\n",
        "                model=\"text-embedding-ada-002\"\n",
        "            )\n",
        "            return np.array(response.data[0].embedding).reshape(1, -1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting embedding for meal '{meal}': {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    meal_embeddings = [get_meal_embedding(meal) for meal in meal_descriptions]\n",
        "    \n",
        "    portion_sizes = []\n",
        "    for meal, meal_embed, meal_weight in zip(meal_descriptions, meal_embeddings, meal_weights):\n",
        "        if meal_embed is None:\n",
        "            continue\n",
        "        # Calculate cosine similarity\n",
        "        similarities = conversion_df['embedding'].apply(lambda x: cosine_similarity(meal_embed, np.array(x).reshape(1, -1))[0][0])\n",
        "        \n",
        "        # Find the index of the most similar row\n",
        "        closest_match_index = similarities.idxmax()\n",
        "        closest_match = conversion_df.loc[closest_match_index]\n",
        "\n",
        "        print(closest_match['food_description'], meal)\n",
        "\n",
        "        # Handle null quantity\n",
        "        quantity = closest_match['quantity']\n",
        "\n",
        "        if pd.isna(quantity):\n",
        "            quantity = 1\n",
        "        \n",
        "        # Calculate the portion size in natural units\n",
        "        conversion_factor = quantity / closest_match['grams']\n",
        "        natural_unit_portion = meal_weight * conversion_factor\n",
        "\n",
        "        natural_unit_portion = round_to_fraction(natural_unit_portion)\n",
        "\n",
        "        clean_natural_unit = re.sub(r'\\d+', '', closest_match['natural_unit']).strip()\n",
        "        \n",
        "        if natural_unit_portion == \"0\":\n",
        "            portion_sizes.append(f\"a pinch\")\n",
        "        else:\n",
        "            portion_sizes.append(f\"{natural_unit_portion} {clean_natural_unit}\")\n",
        "    \n",
        "    return portion_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_ingredient(ingredient):\n",
        "    # Convert to lowercase\n",
        "    ingredient = ingredient.lower()\n",
        "    # Remove extra whitespace\n",
        "    ingredient = re.sub(r'\\s+', ' ', ingredient).strip()\n",
        "    # Remove any non-alphanumeric characters except spaces\n",
        "    ingredient = re.sub(r'[^a-z0-9 ]', '', ingredient)\n",
        "    return ingredient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tea, hot, with milk Tea with Milk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rice pilaf Pilau\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gizzard Githeri\n",
            "Cucumber and vegetable namasu Sukuma Wiki\n",
            "Fufu Ugali\n",
            "Lomi salmon Nyama Choma\n",
            "Bread, chappatti or roti Chapati\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coffee and chicory, brewed Kenyan Chai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fufu Ugali\n",
            "Moose Maziwa Lala\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Milk, condensed, sweetened Sweetened Milk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fufu Ugali\n",
            "Cucumber and vegetable namasu Sukuma Wiki\n",
            "Beans, liquid from stewed kidney beans Bean Stew\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fufu Ugali\n",
            "Cucumber and vegetable namasu Sukuma Wiki\n",
            "Vegetarian stew Vegetable Stew\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tea, iced, brewed, green, pre-sweetened with sugar Sweet Tea\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fufu Ugali\n",
            "Cucumber and vegetable namasu Sukuma Wiki\n",
            "Seaweed soup Ewedu Soup\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SILK Chai, soymilk Chai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qianc\\AppData\\Local\\Temp\\ipykernel_25904\\2538340784.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[70], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m             meal_fiber[meal_index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIBTG_g\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[ind]\n\u001b[0;32m     53\u001b[0m             meal_calories[meal_index] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENERGY_kcal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[ind]\n\u001b[1;32m---> 55\u001b[0m portion_weights \u001b[38;5;241m=\u001b[39m \u001b[43mportion_weight_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeal_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeal_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m meal_carbs_list \u001b[38;5;241m=\u001b[39m meal_carbs\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     58\u001b[0m meal_protein_list \u001b[38;5;241m=\u001b[39m meal_protein\u001b[38;5;241m.\u001b[39mtolist()\n",
            "Cell \u001b[1;32mIn[68], line 24\u001b[0m, in \u001b[0;36mportion_weight_conversion\u001b[1;34m(meal_descriptions, meal_weights)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeal_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Find the index of the most similar row\u001b[39;00m\n\u001b[0;32m     27\u001b[0m closest_match_index \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39midxmax()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[68], line 24\u001b[0m, in \u001b[0;36mportion_weight_conversion.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m similarities \u001b[38;5;241m=\u001b[39m conversion_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeal_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Find the index of the most similar row\u001b[39;00m\n\u001b[0;32m     27\u001b[0m closest_match_index \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39midxmax()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:1685\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1683\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m X_normalized\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1685\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1687\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\preprocessing\\_data.py:1933\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     sparse_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1931\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m-> 1933\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthe normalize function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1942\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1075\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m-> 1075\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[0;32m   1081\u001b[0m         array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m   1082\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:743\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 743\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Group the DataFrame\n",
        "grouped_df = combined_df.groupby(['SUBJECT', 'WEEK_DAY', 'MEAL_NAME'])\n",
        "# print(grouped_df.first())\n",
        "\n",
        "# Process the grouped data\n",
        "for (subject, week_day, meal_name), group in grouped_df:\n",
        "\n",
        "    # Meal_type convert into breakfast,lunch,dinner,etc. \n",
        "    meal_type = group['MEAL_NAME'].iloc[0]\n",
        "    meal_type_name = meal_type_dict[meal_name]\n",
        "\n",
        "    # Full Meal Ingredients List (Cleaned)\n",
        "    ingredients = [ingredient.replace('\\t', ' ').strip() for ingredient in group['INGREDIENT_ENG'].tolist()]\n",
        "    ingredients = [clean_ingredient(ingredient) for ingredient in ingredients]\n",
        "\n",
        "    # Create prompt for GPT to identify each dish from the meal based on ingredients, country, and meal type\n",
        "    prompt = f\"You are a helpful assistant that converts ingredient lists into dish names. Given the following ingredients, create a table of dishes commonly consumed in Kenya. Ensure that each dish uses unique ingredients, meaning no ingredient appears in more than one dish. Provide a table with two columns: 'Dish' and 'Ingredients' and nothing else. The ingredients do not need to be part of traditional Kenyan dishes, but they were consumed in Kenya. \\n \\n Ingredients: {ingredients} \\n Consumted at: {meal_type_dict[meal_name]}\"\n",
        "    serving_sizes = group['FOOD_AMOUNT_REPORTED'].tolist()\n",
        "    #print(prompt)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    meal_name = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Separate the meal into separate dishes and then parse the ingredients from each dish\n",
        "    parsed_meal_name = parse_table_to_dict(meal_name)\n",
        "    meal_description = list(parsed_meal_name.keys()) # List of dish names\n",
        "    meal_recipe = [[clean_ingredient(ingredient) for ingredient in recipe] for recipe in parsed_meal_name.values()] # List of ingredients for each dish\n",
        "    source = 'WHO'\n",
        "    country = 'Kenya'\n",
        "\n",
        "    meal_weights = np.zeros(len(meal_description)) # Initialize the weight of each dish to 0\n",
        "    meal_carbs = np.zeros(len(meal_description))\n",
        "    meal_protein = np.zeros(len(meal_description))\n",
        "    meal_fat = np.zeros(len(meal_description))\n",
        "    meal_fiber = np.zeros(len(meal_description))\n",
        "    meal_calories = np.zeros(len(meal_description))\n",
        "\n",
        "    # Add ingredients to the respective dishes\n",
        "    for ind, ingredient in enumerate(ingredients):\n",
        "        for meal_index, meal in enumerate(meal_recipe):\n",
        "            if ingredient in meal:\n",
        "                meal_weights[meal_index] += serving_sizes[ind]\n",
        "                meal_carbs[meal_index] += group['CARBOH_g'].iloc[ind]\n",
        "                meal_protein[meal_index] += group['PROTEIN_g'].iloc[ind]\n",
        "                meal_fat[meal_index] += group['FAT_g'].iloc[ind]\n",
        "                meal_fiber[meal_index] += group['FIBTG_g'].iloc[ind]\n",
        "                meal_calories[meal_index] += group['ENERGY_kcal'].iloc[ind]\n",
        "\n",
        "    portion_weights = portion_weight_conversion(meal_description, meal_weights)\n",
        "\n",
        "    meal_carbs_list = meal_carbs.tolist()\n",
        "    meal_protein_list = meal_protein.tolist()\n",
        "    meal_fat_list = meal_fat.tolist()\n",
        "    meal_fiber_list = meal_fiber.tolist()\n",
        "    meal_calories_list = meal_calories.tolist()\n",
        "\n",
        "    # Prepare data for writing to CSV\n",
        "    row_data = {\n",
        "        'Meal Description': meal_description,\n",
        "        'Meal Name': meal_recipe,\n",
        "        'Serving Sizes': portion_weights,\n",
        "        'Meal Type': meal_type_name,\n",
        "        'Country': country,\n",
        "        'Source': source,\n",
        "        'Weight': meal_weights,\n",
        "        'Carb': meal_carbs_list,\n",
        "        'Protein': meal_protein_list,\n",
        "        'Fat': meal_fat_list,\n",
        "        'Fiber': meal_fiber_list,\n",
        "        'Calories': meal_calories_list\n",
        "    }\n",
        "\n",
        "    # Append the row to the CSV file\n",
        "    with open('grouped_meals_final.csv', 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['Meal Description', 'Meal Name', 'Serving Sizes', 'Meal Type', 'Country', 'Source', 'Weight', 'Carb', 'Protein', 'Fat', 'Fiber', 'Calories'])\n",
        "        if f.tell() == 0:  # If file is empty, write header\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row_data)\n",
        "    # print(subject, week_day, meal_name, total_carbs, serving_sizes, group['INGREDIENT_ENG'].tolist())\n",
        "\n",
        "\n",
        "print(\"Grouped data has been processed and appended to 'grouped_meals.csv'.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
